{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb\n",
    "\n",
    "RND_STATE = 874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.3\n",
      "IPython 5.3.0\n",
      "\n",
      "numpy 1.14.2\n",
      "scipy 1.0.0\n",
      "pandas 0.20.1\n",
      "matplotlib 2.0.2\n",
      "statsmodels 0.8.0\n",
      "sklearn 0.19.1\n",
      "xgboost 0.7\n",
      "seaborn 0.8.1\n",
      "\n",
      "compiler   : MSC v.1900 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   :\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,statsmodels,sklearn,xgboost,seaborn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('contest_train.csv')\n",
    "test_data = pd.read_csv('contest_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns by datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop = ['FEATURE_3', 'FEATURE_144', 'FEATURE_249', 'FEATURE_256'] # на удаление\n",
    "\n",
    "col_redundant = ['FEATURE_117', 'FEATURE_118', 'FEATURE_149',\n",
    "                 'FEATURE_150', 'FEATURE_155', 'FEATURE_163',\n",
    "                 'FEATURE_164', 'FEATURE_168', 'FEATURE_182',\n",
    "                 'FEATURE_184', 'FEATURE_203', 'FEATURE_205',\n",
    "                 'FEATURE_206', 'FEATURE_227', 'FEATURE_241',\n",
    "                 'FEATURE_37', 'FEATURE_49', 'FEATURE_55',\n",
    "                 'FEATURE_61', 'FEATURE_67', 'FEATURE_68',\n",
    "                 'FEATURE_71', 'FEATURE_72', 'FEATURE_73',\n",
    "                 'FEATURE_78', 'FEATURE_79', 'FEATURE_81',\n",
    "                 'FEATURE_84'] # сильная корреляция с другими, на удаление\n",
    "\n",
    "col_num =  ['FEATURE_7', 'FEATURE_8', 'FEATURE_12',\n",
    "            'FEATURE_21', 'FEATURE_23', 'FEATURE_24',\n",
    "            'FEATURE_26', 'FEATURE_33', 'FEATURE_34',\n",
    "            'FEATURE_35', 'FEATURE_36', 'FEATURE_38',\n",
    "            'FEATURE_45', 'FEATURE_46', 'FEATURE_47',\n",
    "            'FEATURE_48', 'FEATURE_50', 'FEATURE_51',\n",
    "            'FEATURE_52', 'FEATURE_53', 'FEATURE_54',\n",
    "            'FEATURE_56', 'FEATURE_57', 'FEATURE_58',\n",
    "            'FEATURE_59', 'FEATURE_60', 'FEATURE_62',\n",
    "            'FEATURE_63', 'FEATURE_64', 'FEATURE_65',\n",
    "            'FEATURE_66', 'FEATURE_77', 'FEATURE_80',\n",
    "            'FEATURE_82', 'FEATURE_83', 'FEATURE_85',\n",
    "            'FEATURE_86', 'FEATURE_87', 'FEATURE_88',\n",
    "            'FEATURE_89', 'FEATURE_90', 'FEATURE_91',\n",
    "            'FEATURE_92', 'FEATURE_93', 'FEATURE_94',\n",
    "            'FEATURE_95', 'FEATURE_96', 'FEATURE_97',\n",
    "            'FEATURE_98', 'FEATURE_99', 'FEATURE_100',\n",
    "            'FEATURE_101', 'FEATURE_102', 'FEATURE_103',\n",
    "            'FEATURE_104', 'FEATURE_105', 'FEATURE_106',\n",
    "            'FEATURE_107', 'FEATURE_108', 'FEATURE_109',\n",
    "            'FEATURE_112', 'FEATURE_113', 'FEATURE_127',\n",
    "            'FEATURE_128', 'FEATURE_129', 'FEATURE_130',\n",
    "            'FEATURE_135', 'FEATURE_136', 'FEATURE_137',\n",
    "            'FEATURE_138', 'FEATURE_143', 'FEATURE_147',\n",
    "            'FEATURE_160', 'FEATURE_161', 'FEATURE_162',\n",
    "            'FEATURE_177', 'FEATURE_181', 'FEATURE_183',\n",
    "            'FEATURE_186', 'FEATURE_188', 'FEATURE_195',\n",
    "            'FEATURE_217', 'FEATURE_223', 'FEATURE_225',\n",
    "            'FEATURE_228', 'FEATURE_230', 'FEATURE_232',\n",
    "            'FEATURE_233', 'FEATURE_235', 'FEATURE_236',\n",
    "            'FEATURE_237', 'FEATURE_240', 'FEATURE_244',\n",
    "            'FEATURE_252', 'FEATURE_253', 'FEATURE_0', \n",
    "            'FEATURE_13', 'FEATURE_22', 'FEATURE_25', \n",
    "            'FEATURE_39', 'FEATURE_40', 'FEATURE_41', \n",
    "            'FEATURE_42', 'FEATURE_43', 'FEATURE_44', \n",
    "            'FEATURE_76', 'FEATURE_110', 'FEATURE_111', \n",
    "            'FEATURE_114', 'FEATURE_115', 'FEATURE_116', \n",
    "            'FEATURE_119', 'FEATURE_120', 'FEATURE_121', \n",
    "            'FEATURE_122', 'FEATURE_124', 'FEATURE_125', \n",
    "            'FEATURE_126', 'FEATURE_132', 'FEATURE_133', \n",
    "            'FEATURE_134', 'FEATURE_139', 'FEATURE_142', \n",
    "            'FEATURE_145', 'FEATURE_148', 'FEATURE_153', \n",
    "            'FEATURE_158', 'FEATURE_166', 'FEATURE_169', \n",
    "            'FEATURE_171', 'FEATURE_173', 'FEATURE_174', \n",
    "            'FEATURE_185', 'FEATURE_196', 'FEATURE_197', \n",
    "            'FEATURE_215', 'FEATURE_216', 'FEATURE_221', \n",
    "            'FEATURE_222', 'FEATURE_226', 'FEATURE_231', \n",
    "            'FEATURE_234', 'FEATURE_238', 'FEATURE_239', \n",
    "            'FEATURE_242', 'FEATURE_243', 'FEATURE_245', \n",
    "            'FEATURE_246', 'FEATURE_247', 'FEATURE_248', \n",
    "            'FEATURE_250', 'FEATURE_251', 'FEATURE_141',\n",
    "            'FEATURE_199', 'FEATURE_20', 'FEATURE_224'] # числовые переменные\n",
    "\n",
    "col_cat = ['FEATURE_1', 'FEATURE_9', 'FEATURE_10',\n",
    "           'FEATURE_14', 'FEATURE_27','FEATURE_229',\n",
    "           'FEATURE_28', 'FEATURE_29', 'FEATURE_30',\n",
    "           'FEATURE_31', 'FEATURE_32', 'FEATURE_69',\n",
    "           'FEATURE_70', 'FEATURE_74', 'FEATURE_75',\n",
    "           'FEATURE_123', 'FEATURE_131', 'FEATURE_259',\n",
    "           'FEATURE_146', 'FEATURE_151', 'FEATURE_152',\n",
    "           'FEATURE_154', 'FEATURE_156', 'FEATURE_157',\n",
    "           'FEATURE_165', 'FEATURE_167', 'FEATURE_170',\n",
    "           'FEATURE_172', 'FEATURE_175', 'FEATURE_176',\n",
    "           'FEATURE_178', 'FEATURE_179', 'FEATURE_180',\n",
    "           'FEATURE_198', 'FEATURE_200', 'FEATURE_258',\n",
    "           'FEATURE_201', 'FEATURE_202', 'FEATURE_204',\n",
    "           'FEATURE_207', 'FEATURE_208', 'FEATURE_209',\n",
    "           'FEATURE_210', 'FEATURE_211', 'FEATURE_212',\n",
    "           'FEATURE_213', 'FEATURE_214', 'FEATURE_218',\n",
    "           'FEATURE_219', 'FEATURE_220', 'FEATURE_257'] # категориальные переменные\n",
    "\n",
    "\n",
    "col_new_bin = ['FEATURE_189', 'FEATURE_194', 'FEATURE_190', \n",
    "               'FEATURE_191', 'FEATURE_192', 'FEATURE_193', \n",
    "               'FEATURE_187'] # новые бинарные, где очень много NaN\n",
    "\n",
    "col_default_num = ['FEATURE_108', 'FEATURE_98', 'FEATURE_82',\n",
    "                   'FEATURE_25', 'FEATURE_42', 'FEATURE_39',\n",
    "                   'FEATURE_43', 'FEATURE_40', 'FEATURE_44',\n",
    "                   'FEATURE_41', 'FEATURE_119', 'FEATURE_251',\n",
    "                   'FEATURE_120', 'FEATURE_139', 'FEATURE_142',\n",
    "                   'FEATURE_145', 'FEATURE_148', 'FEATURE_141',\n",
    "                   'FEATURE_153', 'FEATURE_158', 'FEATURE_171',\n",
    "                   'FEATURE_185', 'FEATURE_197', 'FEATURE_231',\n",
    "                   'FEATURE_234', 'FEATURE_242', 'FEATURE_250',\n",
    "                   'FEATURE_243', 'FEATURE_245', 'FEATURE_246',\n",
    "                   'FEATURE_247', 'FEATURE_248', 'FEATURE_20',\n",
    "                   'FEATURE_199', 'FEATURE_224'] # числовые, которые не надо преобразовывать\n",
    "\n",
    "col_log_std_num = ['FEATURE_135', 'FEATURE_127', 'FEATURE_107',\n",
    "                   'FEATURE_97', 'FEATURE_87', 'FEATURE_77',\n",
    "                   'FEATURE_22'] # числовые, которые нужно логарифмировать\n",
    "\n",
    "col_extra_num = ['FEATURE_87', 'FEATURE_77', 'FEATURE_244',\n",
    "                 'FEATURE_233', 'FEATURE_232', 'FEATURE_217',\n",
    "                 'FEATURE_177', 'FEATURE_89', 'FEATURE_88', \n",
    "                 'FEATURE_56', 'FEATURE_54', 'FEATURE_53', \n",
    "                 'FEATURE_52', 'FEATURE_51', 'FEATURE_38', \n",
    "                 'FEATURE_36', 'FEATURE_35', 'FEATURE_34', \n",
    "                 'FEATURE_33','FEATURE_12', 'FEATURE_8', \n",
    "                 'FEATURE_7', 'FEATURE_0', 'FEATURE_13',\n",
    "                 'FEATURE_76', 'FEATURE_110', 'FEATURE_111',\n",
    "                 'FEATURE_125', 'FEATURE_133', 'FEATURE_196',\n",
    "                 'FEATURE_226'] # для создания дополниельных признаков, исходя из распределений\n",
    "\n",
    "col_bin = ['FEATURE_2', 'FEATURE_4', 'FEATURE_5',\n",
    "           'FEATURE_6', 'FEATURE_11', 'FEATURE_15',\n",
    "           'FEATURE_16', 'FEATURE_17', 'FEATURE_18',\n",
    "           'FEATURE_19', 'FEATURE_140', 'FEATURE_159',\n",
    "           'FEATURE_254', 'FEATURE_255']\n",
    "\n",
    "col_025_num = [col for col in col_num if col not in col_default_num+col_log_std_num] \n",
    "# числовые, которые нужно преобразовать через корень 4 степени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline = Features Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 35 # SVD\n",
    "\n",
    "n_clusters_1 = 3 # Clustering\n",
    "n_clusters_2 = 5\n",
    "n_clusters_3 = 10\n",
    "\n",
    "n_features = 'all' # SelectKBest\n",
    "\n",
    "levels = np.union1d(train_data[col_cat].fillna(9999), test_data[col_cat].fillna(9999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectColumnsTransfomer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns=[]):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        trans = X[self.columns].copy() \n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class DataFrameFunctionTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, func, impute = False):\n",
    "        self.func = func\n",
    "        self.impute = impute\n",
    "        self.series = pd.Series() \n",
    "\n",
    "    def transform(self, X, **transformparams):\n",
    "        if self.impute:\n",
    "            trans = pd.DataFrame(X).fillna(self.series).copy()\n",
    "        else:\n",
    "            trans = pd.DataFrame(X).apply(self.func).copy()\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        if self.impute:\n",
    "            self.series = pd.DataFrame(X).apply(self.func).copy()\n",
    "        return self\n",
    "    \n",
    "    \n",
    "class DataFrameFeatureUnion(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, list_of_transformers):\n",
    "        self.list_of_transformers = list_of_transformers\n",
    "        \n",
    "    def transform(self, X, **transformparamn):\n",
    "        concatted = pd.concat([transformer.transform(X)\n",
    "                            for transformer in\n",
    "                            self.fitted_transformers_], axis=1).copy()\n",
    "        return concatted\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        self.fitted_transformers_ = []\n",
    "        for transformer in self.list_of_transformers:\n",
    "            fitted_trans = clone(transformer).fit(X, y=None, **fitparams)\n",
    "            self.fitted_transformers_.append(fitted_trans)\n",
    "        return self\n",
    "    \n",
    "\n",
    "class DummiesTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transformparams):\n",
    "        return pd.get_dummies(X).copy()\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataFrameSVDTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transformparams):\n",
    "        trans = X\n",
    "        columns = ['COMPONENT_'+str(i) for i in range(trans.shape[1])]\n",
    "        trans = pd.DataFrame(data=trans, columns=columns)\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        return self \n",
    "    \n",
    "class DataFrameClustTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def transform(self, X, **transformparams):\n",
    "        trans = X\n",
    "        if trans.shape[1]>1:\n",
    "            columns = ['CLUSTER_'+str(trans.shape[1])+'_Dist_'+str(i) for i in range(trans.shape[1])]\n",
    "        else:\n",
    "            columns = ['CLUSTER_'+str(len(np.unique(trans)))]\n",
    "            \n",
    "        trans = pd.DataFrame(data=trans, columns=columns)\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        return self \n",
    "    \n",
    "    \n",
    "class ColumnsNameTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def transform(self, X, **transformparams):\n",
    "        trans = X.copy()\n",
    "        trans.columns = [col + '_extra' for col in X.columns]\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        return self \n",
    "    \n",
    "    \n",
    "class DropAllZeroTrainColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transformparams):       \n",
    "        return X.drop(self.cols_, axis=1).copy()\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        self.cols_ = X.columns[(X==0).all()]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessing_pipeline = [ \n",
    "    \n",
    "                            #select binary\n",
    "                            make_pipeline(  \n",
    "                                SelectColumnsTransfomer(col_bin)),\n",
    "\n",
    "                            #new_binary\n",
    "                            make_pipeline(  \n",
    "                                SelectColumnsTransfomer(col_new_bin),\n",
    "                                DataFrameFunctionTransformer(func = lambda x: np.where(x.isnull(), 0, 1))), \n",
    "\n",
    "                            #numeric default\n",
    "                            make_pipeline(  \n",
    "                                SelectColumnsTransfomer(col_default_num),\n",
    "                                DataFrameFunctionTransformer(lambda x: x.median(), impute=True)), \n",
    "\n",
    "                            #numeric log std\n",
    "                            make_pipeline(  \n",
    "                                SelectColumnsTransfomer(col_log_std_num),\n",
    "                                DataFrameFunctionTransformer(func = lambda x: np.log(x + x.std())),\n",
    "                                DataFrameFunctionTransformer(lambda x: x.median(), impute=True)),\n",
    "\n",
    "                            #numeric **0.25\n",
    "                            make_pipeline(  \n",
    "                                SelectColumnsTransfomer(col_025_num),\n",
    "                                DataFrameFunctionTransformer(func = lambda x: x**0.25),\n",
    "                                DataFrameFunctionTransformer(lambda x: x.median(), impute=True)),\n",
    "\n",
    "                            #categorical\n",
    "                            make_pipeline(  \n",
    "                                SelectColumnsTransfomer(col_cat),\n",
    "                                DataFrameFunctionTransformer(lambda x: 9999, impute=True),\n",
    "                                DataFrameFunctionTransformer(lambda x: x.astype(int)), \n",
    "                                DataFrameFunctionTransformer(lambda x: x.astype('category', categories=levels)),\n",
    "                                DummiesTransformer(),\n",
    "                                DropAllZeroTrainColumnsTransformer(),\n",
    "                                TruncatedSVD(n_components=n_components, random_state=RND_STATE),\n",
    "                                DataFrameSVDTransformer())\n",
    "                             \n",
    "                          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_numeric_pipeline = [ \n",
    "    \n",
    "                                    #select all collumns\n",
    "                                    make_pipeline(\n",
    "                                        SelectColumnsTransfomer(['COMPONENT_'+str(i) for i in range(n_components)] +\n",
    "                                                                 col_num + col_new_bin + col_bin)),\n",
    "                                    #numeric extra features\n",
    "                                    make_pipeline(  \n",
    "                                        SelectColumnsTransfomer(col_extra_num),\n",
    "                                        DataFrameFunctionTransformer(func = lambda x: np.where(x >= x.median(), 1, 0)),\n",
    "                                        ColumnsNameTransformer())\n",
    "    \n",
    "                                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_clustering_pipeline = [\n",
    "    \n",
    "                                    #select all collumns\n",
    "                                    make_pipeline(\n",
    "                                        SelectColumnsTransfomer(['COMPONENT_'+str(i) for i in range(n_components)] +\n",
    "                                                                [col + '_extra' for col in col_extra_num] +\n",
    "                                                                 col_num + col_new_bin + col_bin)),\n",
    "\n",
    "                                    #clusterization  n_clusters = 3\n",
    "                                    make_pipeline(\n",
    "                                        SelectColumnsTransfomer(['COMPONENT_'+str(i) for i in range(n_components)] +\n",
    "                                                                [col + '_extra' for col in col_extra_num] +\n",
    "                                                                 col_num + col_new_bin + col_bin),\n",
    "                                        KMeans(random_state=RND_STATE, n_clusters=n_clusters_1, \n",
    "                                                         precompute_distances = True, n_jobs=-1),\n",
    "                                        DataFrameClustTransformer()),\n",
    "\n",
    "                                    #clusterization  n_clusters = 5\n",
    "                                    make_pipeline(\n",
    "                                        SelectColumnsTransfomer(['COMPONENT_'+str(i) for i in range(n_components)] +\n",
    "                                                                [col + '_extra' for col in col_extra_num] +\n",
    "                                                                 col_num + col_new_bin + col_bin),\n",
    "                                        KMeans(random_state=RND_STATE, n_clusters=n_clusters_2, \n",
    "                                               precompute_distances = True, n_jobs=-1),\n",
    "                                        DataFrameClustTransformer()),\n",
    "\n",
    "                                    #clusterization  n_clusters = 10\n",
    "                                    make_pipeline(\n",
    "                                        SelectColumnsTransfomer(['COMPONENT_'+str(i) for i in range(n_components)] +\n",
    "                                                                [col + '_extra' for col in col_extra_num] +\n",
    "                                                                 col_num + col_new_bin + col_bin),\n",
    "                                        KMeans(random_state=RND_STATE, n_clusters=n_clusters_3, \n",
    "                                               precompute_distances = True, n_jobs=-1),\n",
    "                                        DataFrameClustTransformer())\n",
    "    \n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessing = DataFrameFeatureUnion(preprocessing_pipeline)\n",
    "feature_engineering_numeric = DataFrameFeatureUnion(feature_engineering_numeric_pipeline)\n",
    "#feature_engineering_clustering = DataFrameFeatureUnion(feature_engineering_clustering_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformation = make_pipeline(preprocessing, \n",
    "                                    feature_engineering_numeric,\n",
    "                                    #feature_engineering_clustering,\n",
    "                                    SelectKBest(f_classif, k=n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_data.TARGET\n",
    "train_data_transformed = data_transformation.fit_transform(train_data.iloc[:, 2:], target)\n",
    "test_data_transformed = data_transformation.transform(test_data.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = []\n",
    "for element in target:\n",
    "    if element == 0:\n",
    "        sample_weights.append(0.06)\n",
    "    elif element == 1:\n",
    "        sample_weights.append(0.23)\n",
    "    elif element == 2:\n",
    "        sample_weights.append(0.71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(silent=1, \n",
    "                              colsample_bytree=0.8,\n",
    "                              gamma=0.3, \n",
    "                              max_depth=8, \n",
    "                              min_child_weight=2.0,\n",
    "                              n_estimators=5300, \n",
    "                              subsample=0.7, \n",
    "                              learning_rate=0.066,\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(train_data_transformed, target)]\n",
    "\n",
    "xgb_model.fit(train_data_transformed, target, \n",
    "              sample_weight=sample_weights,\n",
    "              eval_metric=['merror', 'mlogloss'], \n",
    "              eval_set=eval_set, \n",
    "              verbose=False, \n",
    "              early_stopping_rounds=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(test_data_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_answer = test_data[\"ID\"].to_frame()\n",
    "contest_answer[\"TARGET\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_answer.to_csv('contest_answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = xgb_model.predict_proba(test_data_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(y_pred_proba):\n",
    "    best_class = []\n",
    "    best_proba = []\n",
    "    for i, row in enumerate(y_pred_proba):\n",
    "        max_proba = np.max(row)\n",
    "        best_proba.append(max_proba)\n",
    "        best_class.append(row.tolist().index(max_proba))    \n",
    "\n",
    "    list_proba = (list(map(lambda x, y, z: (x, (y, z)), best_class, best_proba, range(len(y_pred_proba)))))\n",
    "    list_proba = sorted(list_proba, key=lambda tup: (tup[0], tup[1][0]), reverse=True)\n",
    "    index_class1_500 = [tup[1][1] for tup in list_proba if tup[0]==1][:500]\n",
    "    index_class2_200 = [tup[1][1] for tup in list_proba if tup[0]==2][:200]\n",
    "    \n",
    "    df_cl1_500 = test_data.ID.iloc[index_class1_500].to_frame()\n",
    "    df_cl2_200 = test_data.ID.iloc[index_class2_200].to_frame()\n",
    "    \n",
    "    df_cl1_500[\"TARGET\"] = 1\n",
    "    df_cl2_200[\"TARGET\"] = 2\n",
    "    \n",
    "    contest_segments = pd.concat([df_cl1_500, df_cl2_200])\n",
    "    return contest_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_segments = get_segments(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_segments.to_csv('contest_segments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
